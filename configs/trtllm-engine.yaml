id: trtllm-engine
base_config:
  model: meta-llama/Llama-3.1-8B-Instruct
  region: us-chicago-1
  gpu: H100
  data: prompt_tokens=512,generated_tokens=128
configs:
  - llm_server_type: sglang
  - llm_server_type: vllm
  - llm_server_type: vllm
    llm_server_config:
      docker_tag: v0.8.1
  - llm_server_type: trtllm
    llm_server_config:
      - version: 0.17.0.post1
      - version: 0.17.0.post1
        llm_kwargs:
          kv_cache_config:
            free_gpu_memory_fraction: 0.1
          build_config:
            plugin_config:
              multiple_profiles: true
              paged_kv_cache: true
              use_paged_context_fmha: true
              gemm_plugin: auto
            max_input_len: 4096
            max_num_tokens: 65536
            max_batch_size: 32
      - version: 0.17.0.post1
        llm_kwargs:
          quant_config:
            quant_algo: FP8
          kv_cache_config:
            free_gpu_memory_fraction: 0.1
          calib_config:
            calib_batches: 512
            calib_batch_size: 1
            calib_max_seq_length: 2048
            tokenizer_max_seq_length: 4096
          build_config:
            plugin_config:
              multiple_profiles: true
              use_paged_context_fmha: true
              low_latency_gemm_swiglu_plugin: fp8
              low_latency_gemm_plugin: fp8
            max_input_len: 32768
            max_num_tokens: 65536
            max_batch_size: 16
          speculative_config:
            max_window_size: 8
            max_ngram_size: 6
            max_verification_set_size: 8
