id: vllm
repeats: 3
base_config:
  region: us-chicago-1
  llm_server_type: vllm
  data:
    - prompt_tokens=256,generated_tokens=4096
    - prompt_tokens=4096,generated_tokens=256
    - prompt_tokens=2048,generated_tokens=2048
configs:
  - model: Qwen/Qwen2.5-Coder-7B-Instruct
    gpu: H100
  - model: meta-llama/Llama-3.3-70B-Instruct
    gpu: H100:2
    llm_server_config:
      extra_args: ["--tensor-parallel-size", "2", "--max-model-len", "8192"]
  - model: meta-llama/Llama-3.3-70B-Instruct
    gpu: H100:4
    llm_server_config:
      extra_args: ["--tensor-parallel-size", "4", "--max-model-len", "8192"]
