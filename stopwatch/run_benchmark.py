import modal

from .db import BenchmarkDefaults
from .resources import app, hf_secret, results_volume
from .llm_server import llm_server


MAX_SECONDS_PER_BENCHMARK = 120  # 2 minutes
RESULTS_PATH = "/results"
SCALEDOWN_WINDOW = 5  # 5 seconds
TIMEOUT = 30 * 60  # 30 minutes

benchmarking_image = (
    modal.Image.debian_slim()
    .apt_install("git")
    .pip_install(
        "git+https://github.com/neuralmagic/guidellm.git@55c65c4",
        "prometheus-client",
        "SQLAlchemy",
        "tiktoken",
    )
)

with benchmarking_image.imports():
    from typing import Any, Mapping, Optional
    import asyncio
    import time

    from guidellm.backend import Backend
    from guidellm.executor import Executor
    from guidellm.request import EmulatedRequestGenerator
    from guidellm.request.base import RequestGenerator


def benchmark_runner_cls(region: str):
    def decorator(cls):
        return app.cls(
            image=benchmarking_image,
            secrets=[hf_secret],
            volumes={RESULTS_PATH: results_volume},
            cpu=4,
            memory=2048,
            scaledown_window=SCALEDOWN_WINDOW,
            timeout=TIMEOUT,
            region=region,
        )(cls)

    return decorator


async def _run_executor_for_result(executor):
    report = None

    async for result in executor.run():
        if result.completed:
            report = result.report
            break

    if not report:
        raise ValueError("No report generated by executor")

    return report


class BenchmarkRunner:
    @modal.method()
    def run_benchmark(
        self,
        model: str,
        llm_server_type: str,
        rate_type: str,
        data: str,
        gpu: str = BenchmarkDefaults.GPU,
        region: str = BenchmarkDefaults.REGION,
        llm_server_config: Optional[Mapping[str, Any]] = None,
        rate: Optional[float] = None,
        repeat_index: int = 0,
    ):
        """Benchmarks a LLM deployment on Modal.

        Args:
            model (str, required): Name of the model to benchmark.
            llm_server_type (str): The server to use for benchmarking, either
                'vllm' or 'trtllm'.
            rate_type (str): The type of rate to use for benchmarking, either
                'constant', 'synchronous', or 'throughput'.
            data (str): A configuration for emulated data (e.g.:
                'prompt_tokens=128,generated_tokens=128').
            gpu (str): The GPU to use for benchmarking.
            region (str): The region to use for benchmarking.
            llm_server_config (dict): Configuration for the LLM server.
            rate (float): If rate_type is 'constant', optionally specify the
                number of requests that should be made per second.
            repeat_index (int): The index of the repeat to run.
        """

        if llm_server_type not in BenchmarkDefaults.LLM_SERVER_CONFIGS:
            raise ValueError(
                f"Invalid value for llm_server: {llm_server_type}. Must be one of {BenchmarkDefaults.LLM_SERVER_CONFIGS.keys()}"
            )
        elif llm_server_config is None:
            llm_server_config = BenchmarkDefaults.LLM_SERVER_CONFIGS[llm_server]

        extra_query = {
            "model": model,
            # Include caller_id in extra_query to ensure that similar benchmark
            # runs are given separate vLLM server instances
            "caller_id": modal.current_function_call_id(),
        }

        # TODO: Make vllm a constant
        if llm_server_type == "vllm":
            if llm_server_config.get("extra_args", []) > 0:
                extra_query["extra_llm_args"] = " ".join(
                    llm_server_config["extra_args"]
                )
            if len(llm_server_config.get("env_vars", {})) > 0:
                extra_query["llm_env_vars"] = " ".join(
                    f"{k}={v}" for k, v in llm_server_config["env_vars"].items()
                )

        # Start LLM server in background
        with llm_server(
            llm_server_type=llm_server_type,
            llm_server_config=llm_server_config,
            extra_query=extra_query,
            gpu=gpu,
            region=region,
        ) as llm_server_url:
            # Create backend
            backend_inst = Backend.create(
                backend_type="openai_server",
                target=f"{llm_server_url}/v1",
                model=model,
                extra_query=extra_query,
            )

            request_generator: RequestGenerator

            # Create tokenizer and request generator
            try:
                tokenizer_inst = backend_inst.model_tokenizer()
            except Exception as err:
                raise ValueError("Could not load model's tokenizer") from err

            retries = 0
            while True:
                try:
                    request_generator = EmulatedRequestGenerator(
                        config=data, tokenizer=tokenizer_inst
                    )
                    break
                except Exception as err:
                    print(f"Error creating request generator: {err}")
                    retries += 1
                    time.sleep(min(2**retries, 60))

            # Create executor
            executor = Executor(
                backend=backend_inst,
                request_generator=request_generator,
                mode=rate_type,
                rate=rate if rate_type in ("constant", "poisson") else None,
                max_duration=MAX_SECONDS_PER_BENCHMARK,
            )

            # Run executor
            print(
                "Running executor with args: {}",
                {
                    "mode": rate_type,
                    "rate": rate,
                },
            )

            report = asyncio.run(_run_executor_for_result(executor))
            return report.benchmarks[0].model_dump()


@benchmark_runner_cls(region="us-ashburn-1")
class BenchmarkRunner_OCI_USASHBURN1(BenchmarkRunner):
    pass


@benchmark_runner_cls(region="us-chicago-1")
class BenchmarkRunner_OCI_USCHICAGO1(BenchmarkRunner):
    pass


@benchmark_runner_cls(region="us-east-1")
class BenchmarkRunner_AWS_USEAST1(BenchmarkRunner):
    pass


@benchmark_runner_cls(region="us-east4")
class BenchmarkRunner_GCP_USEAST4(BenchmarkRunner):
    pass


all_benchmark_runner_classes = {
    "us-ashburn-1": BenchmarkRunner_OCI_USASHBURN1,
    "us-east-1": BenchmarkRunner_AWS_USEAST1,
    "us-east4": BenchmarkRunner_GCP_USEAST4,
    "us-chicago-1": BenchmarkRunner_OCI_USCHICAGO1,
}
