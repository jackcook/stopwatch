import modal

from .db import BenchmarkDefaults
from .resources import app, hf_secret, results_volume
from .vllm_runner import vllm


MAX_SECONDS_PER_BENCHMARK = 120  # 2 minutes
RESULTS_PATH = "/results"
SCALEDOWN_WINDOW = 5  # 5 seconds
TIMEOUT = 30 * 60  # 30 minutes

benchmarking_image = (
    modal.Image.debian_slim()
    .apt_install("git")
    .pip_install(
        "git+https://github.com/neuralmagic/guidellm.git@55c65c4",
        "prometheus-client",
        "SQLAlchemy",
        "tiktoken",
    )
)

with benchmarking_image.imports():
    from typing import Dict, List, Optional
    import asyncio
    import time
    import urllib.parse

    from guidellm.backend import Backend
    from guidellm.executor import Executor
    from guidellm.request import EmulatedRequestGenerator
    from guidellm.request.base import RequestGenerator

    from .custom_metrics import vllm_monkey_patch


def benchmark_runner_cls(region: str):
    def decorator(cls):
        return app.cls(
            image=benchmarking_image,
            secrets=[hf_secret],
            volumes={RESULTS_PATH: results_volume},
            cpu=4,
            memory=2048,
            scaledown_window=SCALEDOWN_WINDOW,
            timeout=TIMEOUT,
            region=region,
        )(cls)

    return decorator


async def _run_executor_for_result(executor):
    report = None

    async for result in executor.run():
        if result.completed:
            report = result.report
            break

    if not report:
        raise ValueError("No report generated by executor")

    return report


class BenchmarkRunner:
    @modal.method()
    def run_benchmark(
        self,
        model: str,
        rate_type: str,
        data: str,
        gpu: str = BenchmarkDefaults.GPU,
        region: str = BenchmarkDefaults.REGION,
        vllm_docker_tag: str = BenchmarkDefaults.VLLM_DOCKER_TAG,
        vllm_env_vars: Dict[str, str] = BenchmarkDefaults.VLLM_ENV_VARS,
        vllm_extra_args: List[str] = BenchmarkDefaults.VLLM_EXTRA_ARGS,
        rate: Optional[float] = None,
        repeat_index: int = 0,
    ):
        """Benchmarks a vLLM deployment on Modal.

        Args:
            model (str, required): Name of the model to benchmark.
            data (str): A configuration for emulated data (e.g.:
                'prompt_tokens=128,generated_tokens=128').
            gpu (str): The GPU to use for benchmarking.
            region (str): The region to use for benchmarking.
            vllm_docker_tag (str): Tag of the vLLM server docker image. Defaults to
                `latest`.
            vllm_env_vars (dict): Environment variables to pass to the vLLM server.
            vllm_extra_args (list): Extra arguments to pass to the vLLM server.
        """

        extra_query = {
            "model": model,
            # Include caller_id in extra_query to ensure that similar benchmark
            # runs are given separate vLLM server instances
            "caller_id": modal.current_function_call_id(),
        }

        if len(vllm_extra_args) > 0:
            extra_query["extra_vllm_args"] = " ".join(vllm_extra_args)
        if len(vllm_env_vars) > 0:
            extra_query["vllm_env_vars"] = " ".join(
                f"{k}={v}" for k, v in vllm_env_vars.items()
            )

        # Start vLLM server in background
        with vllm(
            docker_tag=vllm_docker_tag,
            extra_query=extra_query,
            gpu=gpu,
            region=region,
        ) as vllm_url:
            extra_query_args = urllib.parse.urlencode(extra_query)
            metrics_url = f"{vllm_url}/metrics?{extra_query_args}"
            vllm_monkey_patch(metrics_url)

            # Create backend
            backend_inst = Backend.create(
                backend_type="openai_server",
                target=f"{vllm_url}/v1",
                model=model,
                extra_query=extra_query,
            )

            request_generator: RequestGenerator

            # Create tokenizer and request generator
            try:
                tokenizer_inst = backend_inst.model_tokenizer()
            except Exception as err:
                raise ValueError("Could not load model's tokenizer") from err

            retries = 0
            while True:
                try:
                    request_generator = EmulatedRequestGenerator(
                        config=data, tokenizer=tokenizer_inst
                    )
                    break
                except Exception as err:
                    print(f"Error creating request generator: {err}")
                    retries += 1
                    time.sleep(min(2**retries, 60))

            # Create executor
            executor = Executor(
                backend=backend_inst,
                request_generator=request_generator,
                mode=rate_type,
                rate=rate if rate_type in ("constant", "poisson") else None,
                max_duration=MAX_SECONDS_PER_BENCHMARK,
            )

            # Run executor
            print(
                "Running executor with args: {}",
                {
                    "mode": rate_type,
                    "rate": rate,
                },
            )

            report = asyncio.run(_run_executor_for_result(executor))
            return report.benchmarks[0].model_dump()


@benchmark_runner_cls(region="us-ashburn-1")
class BenchmarkRunner_OCI_USASHBURN1(BenchmarkRunner):
    pass


@benchmark_runner_cls(region="us-chicago-1")
class BenchmarkRunner_OCI_USCHICAGO1(BenchmarkRunner):
    pass


@benchmark_runner_cls(region="us-east-1")
class BenchmarkRunner_AWS_USEAST1(BenchmarkRunner):
    pass


@benchmark_runner_cls(region="us-east4")
class BenchmarkRunner_GCP_USEAST4(BenchmarkRunner):
    pass


all_benchmark_runner_classes = {
    "us-ashburn-1": BenchmarkRunner_OCI_USASHBURN1,
    "us-east-1": BenchmarkRunner_AWS_USEAST1,
    "us-east4": BenchmarkRunner_GCP_USEAST4,
    "us-chicago-1": BenchmarkRunner_OCI_USCHICAGO1,
}
